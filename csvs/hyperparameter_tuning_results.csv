exp_id,exp_name,hypothesis,parameter,value,baseline_value,test_loss,test_accuracy,perplexity,training_time_minutes,best_epoch,total_params,perplexity_change,time_change,conclusion,perplexity_improvement_%,time_change_%
EXP-007,Higher Learning Rate,Increasing LR to 0.01 will speed up convergence,learning_rate,0.01,0.001,6.271361351013184,0.08511897176504135,529.1973110211767,2.0928380648295084,15,5458448,8.26729253271401,0.34533366839090984,✗ REJECT: Perplexity increased by 8.2673,-1.587025558001532,19.761533595835147
EXP-008,Smaller Batch Size,Batch size 64 will provide more frequent updates and better generalization,batch_size,64.0,128.0,6.687074661254883,0.042855266481637955,801.9727752795675,3.416846454143524,15,5458448,281.0427567911048,1.6693420577049256,✗ REJECT: Perplexity increased by 281.0428,-53.95019423272671,95.5272021693927
EXP-005,Very High Dropout,Dropout at 0.5 will prevent overfitting but may underfitthe model,dropout,0.5,0.2,6.701236724853516,0.04515577852725983,813.4111691814944,2.1267088850339255,15,5458448,292.48115069303174,0.37920448859532696,✗ REJECT: Perplexity increased by 292.4812,-56.14595825015015,21.699773080293415
EXP-002,Increased Layers,Increasing layers to 3 will improve model capacity and reduce perplexity,layers,3.0,2.0,6.706570148468018,0.04515577852725983,817.7610250203182,2.663711428642273,15,5589776,296.83100653185556,0.9162070322036744,✗ REJECT: Perplexity increased by 296.8310,-56.98097556234987,52.42945506007869
EXP-004,Higher Dropout,Increasing dropout to 0.3 will reduce overfitting and improve generalization,dropout,0.3,0.2,6.714890956878662,0.04515577852725983,824.5938457081602,2.1015640536944074,15,5458448,303.6638272196975,0.3540596572558088,✗ REJECT: Perplexity increased by 303.6638,-58.29263364411451,20.260873619395742
EXP-003,Lower Dropout,Reducing dropout to 0.1 will allow model to learn more complex patterns,dropout,0.1,0.2,6.725620746612549,0.04515577852725983,833.489201595437,2.0973810156186423,14,5458448,312.5591831069743,0.34987661918004376,✗ REJECT: Perplexity increased by 312.5592,-60.00022498490297,20.021501513420496
EXP-001,Reduced Layers,Reducing layers to 1 will speed up training with minimal accuracy loss,layers,1.0,2.0,6.725718975067139,0.04515577852725983,833.5710779728562,1.585176165898641,15,5327120,312.6410594843935,-0.1623282305399576,✗ REJECT: Perplexity increased by 312.6411,-60.01594233166998,-9.289145759563258
EXP-006,Lower Learning Rate,Reducing LR to 0.0001 will allow more careful optimization,learning_rate,0.0001,0.001,6.767106056213379,0.04515577852725983,868.794011411525,2.0719732443491616,15,5458448,347.86399292306237,0.3244688479105631,✗ REJECT: Perplexity increased by 347.8640,-66.77749036855835,18.567555456331228
